{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2_SUBMISSION.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DiqULzOVLmpW",
        "colab_type": "text"
      },
      "source": [
        "**Computer Vision**\n",
        "Juan Álvarez Fernández del Vallado"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpatlOprLwnV",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "The purpose of this lab is to detect a human face using the laptop's camera. To do so,we are going to use the Cascade functions and start from the cv2 repository, available in GitHub.\n",
        "\n",
        "\n",
        "The program will start by analizing all the pixels in the whole image. Once it recognizes a human face, it will square it and save it in a buffer. After a while, it will search for a new face in the surroundings of the square it detected before. This design schema was selected to improve efficiency and timing.\n",
        "\n",
        "The code considers that you have already cloned the opencv repository. Otherwise it can't access the cascade functions and other critical modules.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MF5wavfFKRP",
        "colab_type": "text"
      },
      "source": [
        "# Basic imports"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6D1Wv7emFPq1",
        "colab_type": "text"
      },
      "source": [
        "Firstly we need to import from github the opencv modules.\n",
        "\n",
        "The next step will be to copy the necessary modules to our working space."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDd0UCBHPWDu",
        "colab_type": "code",
        "outputId": "6475c7df-5274-4339-a8c0-56d3998cbca0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "! git clone https://github.com/opencv/opencv\n",
        "! cp /content/opencv/samples/python/common.py .\n",
        "! cp /content/opencv/samples/python/video.py .\n",
        "! cp /content/opencv/modules/python/test/tst_scene_render.py ."
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'opencv'...\n",
            "remote: Enumerating objects: 7, done.\u001b[K\n",
            "remote: Counting objects: 100% (7/7), done.\u001b[K\n",
            "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
            "remote: Total 270871 (delta 1), reused 3 (delta 1), pack-reused 270864\u001b[K\n",
            "Receiving objects: 100% (270871/270871), 468.31 MiB | 30.03 MiB/s, done.\n",
            "Resolving deltas: 100% (189065/189065), done.\n",
            "Checking out files: 100% (6152/6152), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CrylYLE52ooJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Python 2/3 compatibility\n",
        "from __future__ import print_function\n",
        "\n",
        "import numpy as np\n",
        "import cv2 \n",
        "import cv2 as cv\n",
        "from google.colab.patches import cv2_imshow # Needed to print out image\n",
        "\n",
        "# OpenCv imports\n",
        "from common import clock, draw_str\n",
        "import video\n",
        "from video import presets\n",
        "\n",
        "\n",
        "import io\n",
        "import base64\n",
        "\n",
        "\n",
        "\n",
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "from PIL import Image\n",
        "\n",
        "# Debugging\n",
        "import pdb\n",
        "\n",
        "# Histogram imports\n",
        "import sys\n",
        "PY3 = sys.version_info[0] == 3\n",
        "\n",
        "if PY3:\n",
        "    xrange = range\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_rdQAU_EL8T",
        "colab_type": "text"
      },
      "source": [
        "# Laboratory 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aZYsApBJshjH"
      },
      "source": [
        "\n",
        "\n",
        "The purpose of this lab is to detect a human face using the laptop's camera. To do so,we are going to use the Cascade functions and start from cv2 repository,\n",
        "\n",
        "\n",
        "The program will start by analizing all the pixels in the whole image. Once it recognizes a human face, it will square it and save it in a buffer. After a while, it will search for a new face in the surroundings of the square it detected before. This design schema was sleected to improve efficiency and timing.\n",
        "\n",
        "The code considers that you have already cloned the opencv repository. Otherwise it can't access the cascade functions and other critical modules.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6uxgoBBsmRp",
        "colab_type": "text"
      },
      "source": [
        "![picture](https://drive.google.com/uc?id=1JwFe2cXPcqDeKxS6195QshHyd-7SxkxC)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNVXxHrQu9NM",
        "colab_type": "code",
        "outputId": "1538290d-f43c-43af-edbd-2f52b159ed80",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#MAIN PROGRAM. Laboratory 1\n",
        "\n",
        "'''\n",
        "face detection using haar cascades\n",
        "\n",
        "'''\n",
        "\n",
        "# Detection algorithm import\n",
        "# Load cascade file for the detection\n",
        "cascade_fn=\"/content/opencv/data/haarcascades/haarcascade_frontalface_alt.xml\"\n",
        "cascade = cv2.CascadeClassifier(cv2.samples.findFile(cascade_fn))\n",
        "\n",
        "def VideoCapture():\n",
        "  js = Javascript('''\n",
        "    async function create(){\n",
        "      div = document.createElement('div');\n",
        "      document.body.appendChild(div);\n",
        "\n",
        "      video = document.createElement('video');\n",
        "      video.setAttribute('playsinline', '');\n",
        "\n",
        "      div.appendChild(video);\n",
        "\n",
        "      stream = await navigator.mediaDevices.getUserMedia({video: {facingMode: \"environment\"}});\n",
        "      video.srcObject = stream;\n",
        "\n",
        "      await video.play();\n",
        "\n",
        "      canvas =  document.createElement('canvas');\n",
        "      canvas.width = video.videoWidth;\n",
        "      canvas.height = video.videoHeight;\n",
        "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "\n",
        "      div_out = document.createElement('div');\n",
        "      document.body.appendChild(div_out);\n",
        "      img = document.createElement('img');\n",
        "      div_out.appendChild(img);\n",
        "    }\n",
        "\n",
        "    async function capture(){\n",
        "        return await new Promise(function(resolve, reject){\n",
        "            pendingResolve = resolve;\n",
        "            canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "            result = canvas.toDataURL('image/jpeg', 0.8);\n",
        "            pendingResolve(result);\n",
        "        })\n",
        "    }\n",
        "\n",
        "    function showing(imgb64){\n",
        "        img.src = \"data:image/jpg;base64,\" + imgb64;\n",
        "    }\n",
        "\n",
        "  ''')\n",
        "  display(js)\n",
        "\n",
        "# Needed to convert an image to array\n",
        "def byte2image(byte):\n",
        "  jpeg = base64.b64decode(byte.split(',')[1])\n",
        "  im = Image.open(io.BytesIO(jpeg))\n",
        "  return np.array(im)\n",
        "\n",
        "#Inverse operation as above. Needed to show the image.\n",
        "def image2byte(image):\n",
        "  image = Image.fromarray(image)\n",
        "  buffer = io.BytesIO()\n",
        "  image.save(buffer, 'jpeg')\n",
        "  buffer.seek(0)\n",
        "  x = base64.b64encode(buffer.read()).decode('utf-8')\n",
        "  return x\n",
        "  # Function which draws a rectangle in a given photo. It can also select a color and add a label\n",
        "def draw_rects(img, rects, color, Label):\n",
        "    cv2.putText(img, Label, (rects[0][0], rects[0][1]-10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
        "    for x1, y1, x2, y2 in rects:\n",
        "        cv2.rectangle(img, (x1, y1), (x2, y2), color, 2)\n",
        "\n",
        "# This is the function responsible for detecting a face using the cascade classifier.\n",
        "def detect(img, cascade = cascade):\n",
        "    rects = cascade.detectMultiScale(img, scaleFactor=1.3, minNeighbors=4, minSize=(30, 30),\n",
        "                                     flags=cv2.CASCADE_SCALE_IMAGE)\n",
        "    if len(rects) == 0:\n",
        "        return []\n",
        "    rects[:,2:] += rects[:,:2]\n",
        "    return rects\n",
        "\n",
        "#Limit margin of the detected area. Set to 50\n",
        "def setMargin(im, area, margin = 50):\n",
        "    area[0][0] = max(area[0][0]-margin, 0)\n",
        "    area[0][1] = max(area[0][1]-margin, 0)\n",
        "    area[0][2] = min(area[0][2] + margin, im.shape[1])\n",
        "    area[0][3] = min(area[0][3] + margin, im.shape[0])\n",
        "\n",
        "#Transform the points from the previously cropped area\n",
        "def transformation(rects, area):\n",
        "    rects[0][0] += area[0][0]\n",
        "    rects[0][1] += area[0][1]\n",
        "    rects[0][2] += area[0][0]\n",
        "    rects[0][3] += area[0][1]\n",
        "\n",
        "# Initialize the video\n",
        "VideoCapture()\n",
        "eval_js('create()')\n",
        "\n",
        "\n",
        "rects = []\n",
        "area = []\n",
        "while True:\n",
        "  \n",
        "  byte = eval_js('capture()')\n",
        "  im = byte2image(byte)\n",
        "  #rects = cascade.detectMultiScale(im, scaleFactor=1.3, minNeighbors=4, minSize=(30, 30), flags=cv.CASCADE_SCALE_IMAGE)\n",
        "  vis = im.copy()\n",
        "  gray = cv2.cvtColor(im, cv2.COLOR_RGB2GRAY)\n",
        "  \n",
        "  if (rects==[]):\n",
        "    rects = detect(gray, cascade)\n",
        "    # There is no face detected\n",
        "    # We start over\n",
        "    \n",
        "\n",
        "  else:\n",
        "    # Set a margin in case the image position has changed\n",
        "    setMargin(im, area, 50)\n",
        "    #Crop the image with the rectangle from before for detecting the next image in those boundaries.\n",
        "    cropped_gray = gray[area[0][1]:area[0][3], area[0][0]:area[0][2]]\n",
        "    rects = detect(cropped_gray, cascade)\n",
        "    for (x1, y1, x2, y2) in area:\n",
        "      draw_rects(vis, area, (255, 4, 0), \"Rectangle\")\n",
        "      \n",
        "    if rects != []:\n",
        "      # Set points to the real position where they should be\n",
        "      transformation(rects, area)\n",
        "\n",
        "  area = rects\n",
        "\n",
        "  \n",
        "  \n",
        "  vis = im.copy()\n",
        "  \n",
        "  for (x, y, w, h) in rects:\n",
        "    draw_rects(vis, rects, (255, 4, 0), \"Face!\")\n",
        "  \n",
        "  \n",
        "  eval_js('showing(\"{}\")'.format(image2byte(vis)))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function create(){\n",
              "      div = document.createElement('div');\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      video = document.createElement('video');\n",
              "      video.setAttribute('playsinline', '');\n",
              "\n",
              "      div.appendChild(video);\n",
              "\n",
              "      stream = await navigator.mediaDevices.getUserMedia({video: {facingMode: \"environment\"}});\n",
              "      video.srcObject = stream;\n",
              "\n",
              "      await video.play();\n",
              "\n",
              "      canvas =  document.createElement('canvas');\n",
              "      canvas.width = video.videoWidth;\n",
              "      canvas.height = video.videoHeight;\n",
              "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
              "\n",
              "      div_out = document.createElement('div');\n",
              "      document.body.appendChild(div_out);\n",
              "      img = document.createElement('img');\n",
              "      div_out.appendChild(img);\n",
              "    }\n",
              "\n",
              "    async function capture(){\n",
              "        return await new Promise(function(resolve, reject){\n",
              "            pendingResolve = resolve;\n",
              "            canvas.getContext('2d').drawImage(video, 0, 0);\n",
              "            result = canvas.toDataURL('image/jpeg', 0.8);\n",
              "            pendingResolve(result);\n",
              "        })\n",
              "    }\n",
              "\n",
              "    function showing(imgb64){\n",
              "        img.src = \"data:image/jpg;base64,\" + imgb64;\n",
              "    }\n",
              "\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:113: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:128: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-ab491f6a4654>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m   \u001b[0mbyte\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_js\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'capture()'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m   \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbyte2image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbyte\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m   \u001b[0;31m#rects = cascade.detectMultiScale(im, scaleFactor=1.3, minNeighbors=4, minSize=(30, 30), flags=cv.CASCADE_SCALE_IMAGE)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result)\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     if (reply.get('type') == 'colab_reply' and\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRdsdzylFoUA",
        "colab_type": "text"
      },
      "source": [
        "# Laboratory 2.\n",
        "This second laboratory will be based on the camshift software.\n",
        "\n",
        "The imports needed for this part are already included in the first section of this notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lmSHL0IsU4O",
        "colab_type": "text"
      },
      "source": [
        "The following image outputs the **histogram** distribution of my face:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "At_1HqTHuodp",
        "colab_type": "text"
      },
      "source": [
        "![picture](https://drive.google.com/uc?id=19kPNP1-438f2n1eoBni8FBd8Od4hv0gR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRLIHAkjudhE",
        "colab_type": "text"
      },
      "source": [
        "The result of the cam-shift probability image of recognizing my face while moving is displayed below:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUyCRrT5u-P-",
        "colab_type": "text"
      },
      "source": [
        "![picture](https://drive.google.com/uc?id=1kdle79dEsxtUwWL4hOjjsTpFWYe8KH3_)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQd9bJR7F02e",
        "colab_type": "code",
        "outputId": "9bafced4-2a1a-4e52-c421-e5bf5338b8f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "def show_hist(hist):\n",
        "    bin_count = hist.shape[0]\n",
        "    bin_w = 24\n",
        "    img = np.zeros((256, bin_count*bin_w, 3), np.uint8)\n",
        "    for i in xrange(bin_count):\n",
        "        h = int(hist[i])\n",
        "        cv.rectangle(img, (i*bin_w+2, 255), ((i+1)*bin_w-2, 255-h), (int(180.0*i/bin_count), 255, 255), -1)\n",
        "    img = cv.cvtColor(img, cv.COLOR_HSV2BGR)\n",
        "    cv2_imshow(img)\n",
        "\n",
        "\n",
        "\n",
        "def camshift_detection():\n",
        "  selection = []\n",
        "  track_window = []\n",
        "  detected = []\n",
        "\n",
        "\n",
        "  VideoCapture()\n",
        "  eval_js('create()')\n",
        "  while True: \n",
        "    byte = eval_js('capture()')\n",
        "    im = byte2image(byte)\n",
        "    vis = im.copy()\n",
        "    gray = cv2.cvtColor(im, cv2.COLOR_RGB2GRAY)\n",
        "    hsv = cv2.cvtColor(im, cv2.COLOR_BGR2HSV)\n",
        "    mask = cv2.inRange(hsv, np.array((0., 60., 32.)), np.array((180., 255., 255.)))\n",
        "\n",
        "    # The first time. this \"if\" obtains histogram for the first time.\n",
        "    # If no face was detected, new iteration and take new photo to detect the face.\n",
        "    if selection == []:\n",
        "      selection = detect(gray)\n",
        "      #print(selection*)\n",
        "      if selection == []:# we must detect a face or else we won't move forward\n",
        "        continue\n",
        "      else:\n",
        "        #print(type(selection))\n",
        "        x0, y0, x1, y1 = tuple(selection.reshape(1, -1)[0])\n",
        "        # Select just the face region\n",
        "        hsv_roi = hsv[y0:y1, x0:x1]\n",
        "        mask_roi = mask[y0:y1, x0:x1]\n",
        "        # get distribution only one time of a face\n",
        "        hist = cv.calcHist( [hsv_roi], [0], mask_roi, [16], [0, 180] )\n",
        "        cv.normalize(hist, hist, 0, 255, cv.NORM_MINMAX)\n",
        "        hist = hist.reshape(-1)\n",
        "        #Define track window to the face area\n",
        "        track_window= (x0,y0, x1-x0, y1-y0)\n",
        "        show_hist(hist) # We can omit this part\n",
        "\n",
        "        vis_roi = vis[y0:y1, x0:x1]\n",
        "        cv.bitwise_not(vis_roi, vis_roi)\n",
        "        vis[mask==0]=0\n",
        "\n",
        "    if track_window and track_window[2] > 0 and track_window[3] > 0:\n",
        "      # We will come here for the next photo after detecting a face\n",
        "      # Prob will contain the likely areas where the face is (according to histogram)\n",
        "      prob = cv.calcBackProject([hsv], [0], hist, [0, 180], 1)\n",
        "      prob &= mask      \n",
        "      term_crit = ( cv.TERM_CRITERIA_EPS | cv.TERM_CRITERIA_COUNT, 10, 1 )\n",
        "      # Update track window to follow the new face area\n",
        "      track_box, track_window = cv.CamShift(prob, track_window, term_crit)\n",
        "      #print(track_box)\n",
        "      xmin,ymin,dx,dy = track_window  \n",
        "      detectedArea = [(xmin, ymin, xmin+dx, ymin+dy)]  \n",
        "      draw_rects(prob, detectedArea, (255,0,0), 'Detected face')\n",
        "      eval_js('showing(\"{}\")'.format(image2byte(prob)))\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "camshift_detection()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function create(){\n",
              "      div = document.createElement('div');\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      video = document.createElement('video');\n",
              "      video.setAttribute('playsinline', '');\n",
              "\n",
              "      div.appendChild(video);\n",
              "\n",
              "      stream = await navigator.mediaDevices.getUserMedia({video: {facingMode: \"environment\"}});\n",
              "      video.srcObject = stream;\n",
              "\n",
              "      await video.play();\n",
              "\n",
              "      canvas =  document.createElement('canvas');\n",
              "      canvas.width = video.videoWidth;\n",
              "      canvas.height = video.videoHeight;\n",
              "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
              "\n",
              "      div_out = document.createElement('div');\n",
              "      document.body.appendChild(div_out);\n",
              "      img = document.createElement('img');\n",
              "      div_out.appendChild(img);\n",
              "    }\n",
              "\n",
              "    async function capture(){\n",
              "        return await new Promise(function(resolve, reject){\n",
              "            pendingResolve = resolve;\n",
              "            canvas.getContext('2d').drawImage(video, 0, 0);\n",
              "            result = canvas.toDataURL('image/jpeg', 0.8);\n",
              "            pendingResolve(result);\n",
              "        })\n",
              "    }\n",
              "\n",
              "    function showing(imgb64){\n",
              "        img.src = \"data:image/jpg;base64,\" + imgb64;\n",
              "    }\n",
              "\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:34: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEACAIAAAA1FTYwAAAD+UlEQVR4nO3cMWpUURSA4XfDbEKs\nxGUEK3EZYiUuI2QZwUpchliJyxArcRfeFGYkXcD74h8n39fMNPdwp/nfaeaNDR6OD3Pp+Jux0z34\nR87qCwCPlwABGQECMgIEZAQIyAgQkBEgICNAQEaAgIwAARkBAjICBGQECMgIEJARICAjQEBGgICM\nAAEZAQIyAgRkBAjICBCQESAgI0BARoCAjAABGQECMgIEZAQIyAgQkBEgICNAQEaAgIwAARkBAjIC\nBGQECMgIEJARICAjQEBGgICMAAEZAQIyAgRkBAjICBCQESAgI0BARoCAjAABGQECMgIEZAQIyAgQ\nkBEgICNAQEaAgIwAARkBAjICBGQECMgIEJARICAjQEBGgICMAAEZAQIyAgRkBAjICBCQESAgI0BA\nRoCAjAABGQECMgIEZAQIyAgQkBEgICNAQEaAgIwAARkBAjICBGQECMgIEJARICAjQEDmUF8A7sHL\nuXT889jpHtzBBgRkBAjICBCQESAgI0BARoCAjAABGQECMgIEZAQIyAgQkBEgICNAQEaAgIwAARkB\nAjICBGQECMgIEJARICAjQEBGgICMAAEZAQIyAgRkBAjICBCQESAgI0BARoCAjAABGQECMgIEZAQI\nyAgQkBEgICNAQEaAgIwAARkBAjICBGQECMgIEJARICAjQEBGgICMAAEZAQIyAgRkBAjICBCQESAg\nI0BARoCAjAABGQECMgIEZAQIyAgQkBEgICNAQEaAgIwAARkBAjICBGQECMgIEJARICAjQEBGgICM\nAAEZAQIyAgRkBAjICBCQESAgI0BARoCAjAABGQECMgIEZAQIyAgQkBEgICNAQEaAgIwAARkBAjIC\nBGQECMgIEJARICAjQEBGgICMAAEZAQIyAgRkBAjICBCQESAgI0BA5vD7Y65NGesXAR4fGxCQESAg\nI0BARoCAjAABGQECMgIEZAQIyAgQkBEgICNAQEaAgIwAARkBAjICBGQECMgIEJARICAjQEBGgICM\nAAEZAQIyAgRkBAjICBCQESAgI0BARoCAjAABGQECMod9x83XS8fHx53uAfwPbEBARoCAjAABGQEC\nMgIEZAQIyAgQkBEgICNAQEaAgIwAARkBAjICBGR2/jc8nJIn21w5/nMbe93kVNmAgIwAARkBAjIC\nBGQECMgIEJARICAjQEBGgICMAAEZAQIyAgRkBAjICBCQ8ToO4C/N7e3K8bG9twEBGRsQ3LuLtReb\nXZ7ui81sQEDmgW5A89PS8fHq5suPpQfP9vT44LlYm3N5nPNsbc73Pw/Ceb40aHw9zrlam/Pu5suv\ntR92drJP+H19W9uknh83qbl9WZkzthcrx2+7Bp2tIQ5eOD6wAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=384x256 at 0x7FF1470152B0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-0a83224d09d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m \u001b[0mcamshift_detection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-0a83224d09d1>\u001b[0m in \u001b[0;36mcamshift_detection\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m       \u001b[0mdetectedArea\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxmin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mymin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxmin\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mdx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mymin\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mdy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m       \u001b[0mdraw_rects\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetectedArea\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Detected face'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m       \u001b[0meval_js\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'showing(\"{}\")'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage2byte\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result)\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     if (reply.get('type') == 'colab_reply' and\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uupQ7NsTllQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eA8zoCeWUHWb",
        "colab_type": "code",
        "outputId": "7f750c68-61e9-4d05-f483-0834b2c4dedd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "a"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXJDIaIhUlG1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nc-RstwzI8Fo",
        "colab_type": "text"
      },
      "source": [
        "# Laboratory 3\n",
        "This laboratory will be based on the camshift software.\n",
        "Very similarly as in the lab 2, we will use camshift to follow the user's face but instead of marking the face, we will simply remove it by setting an opaque on the face.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRg3yWh-J1wF",
        "colab_type": "text"
      },
      "source": [
        "The following image outputs the result."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWQ7GARfJ5QW",
        "colab_type": "text"
      },
      "source": [
        "![picture](https://drive.google.com/uc?id=1cSm5Nv4J8EiQgO2RaxxuZ0fLyJ_h1cH1)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-WB6ZMUJ0ma",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Mehod to merely paint a white rectangle delimited by region on the image img\n",
        "def draw_rects_filled(img, rects, color, Label):\n",
        "    cv2.putText(img, Label, (rects[0][0], rects[0][1]-10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
        "    for x1, y1, x2, y2 in rects:\n",
        "        cv2.rectangle(img, (x1, y1), (x2, y2), color, cv2.FILLED)\n",
        "\n",
        "\n",
        "# The code in this method is practically the same as in lab2.\n",
        "def camshift_face_removal():\n",
        "  selection = []\n",
        "  track_window = []\n",
        "  detected = []\n",
        "\n",
        "\n",
        "  VideoCapture()\n",
        "  eval_js('create()')\n",
        "  while True: \n",
        "    byte = eval_js('capture()')\n",
        "    im = byte2image(byte)\n",
        "    vis = im.copy()\n",
        "    gray = cv2.cvtColor(im, cv2.COLOR_RGB2GRAY)\n",
        "    hsv = cv2.cvtColor(im, cv2.COLOR_BGR2HSV)\n",
        "    mask = cv2.inRange(hsv, np.array((0., 60., 32.)), np.array((180., 255., 255.)))\n",
        "\n",
        "    # The first time. this \"if\" obtains histogram for the first time\n",
        "    if selection == []:\n",
        "      selection = detect(gray)\n",
        "      #print(selection*)\n",
        "      if selection == []:# we must detect a face or else we won't move forward\n",
        "        continue\n",
        "      else:\n",
        "        #print(type(selection))\n",
        "        x0, y0, x1, y1 = tuple(selection.reshape(1, -1)[0])\n",
        "        #Select the face\n",
        "        hsv_roi = hsv[y0:y1, x0:x1]\n",
        "        mask_roi = mask[y0:y1, x0:x1]\n",
        "        # get distribution only one time of a face\n",
        "        hist = cv.calcHist( [hsv_roi], [0], mask_roi, [16], [0, 180] )\n",
        "        cv.normalize(hist, hist, 0, 255, cv.NORM_MINMAX)\n",
        "        hist = hist.reshape(-1)\n",
        "        # Update track window for the first time to where the face currently is\n",
        "        track_window= (x0,y0, x1-x0, y1-y0)\n",
        "        vis_roi = vis[y0:y1, x0:x1]\n",
        "        cv.bitwise_not(vis_roi, vis_roi)\n",
        "        vis[mask==0]=0\n",
        "\n",
        "    if track_window and track_window[2] > 0 and track_window[3] > 0:\n",
        "      prob = cv.calcBackProject([hsv], [0], hist, [0, 180], 1)\n",
        "      prob &= mask\n",
        "      # prob will contain the pixels where the face is more likely to be\n",
        "      # we apply a filter using mask\n",
        "      term_crit = ( cv.TERM_CRITERIA_EPS | cv.TERM_CRITERIA_COUNT, 10, 1 )\n",
        "      #update track window to where the face probably now is\n",
        "      track_box, track_window = cv.CamShift(prob, track_window, term_crit)\n",
        "      #print(track_box)\n",
        "      xmin,ymin,dx,dy = track_window  \n",
        "      detectedArea = [(xmin, ymin, xmin+dx, ymin+dy)]  \n",
        "      # Paint the detected face\n",
        "      draw_rects_filled(prob, detectedArea, (255,0,0), 'Removed face')\n",
        "      eval_js('showing(\"{}\")'.format(image2byte(prob)))\n",
        "\n",
        "camshift_face_removal()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-G1CB_8KV06",
        "colab_type": "text"
      },
      "source": [
        "# Laboratory 4\n",
        "This laboratory will be based on the camshift software. In this case, we will set an opaque box on the user's face but, additionally, we will also detect the user's hand. Once the hand is the detected, we will follow it using camshift (as in the previous laboratories). Similarly, once the face is covered on a black box, we will also follow it.\n",
        "\n",
        "IMPORTANT: The detection of the hand will merely consist of analyzing the whole image and finding the face histogram, hence we assume the face and the hand's histogram are practically the same. There is no machine learning detecting algorithm for the hand. This greatly reduces the accuracy of this algorithm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BW2cGpd-LQqL",
        "colab_type": "text"
      },
      "source": [
        "![picture](https://drive.google.com/uc?id=191-gy8baG8dbv9IpC283GQLSOa1m9-m3)\n",
        "\n",
        "Even though it may not look clear, in this face there is a face completely covered by a black box.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpLhZWMCPrc7",
        "colab_type": "text"
      },
      "source": [
        "![picture](https://drive.google.com/uc?id=1VXY-sFnurCuZ3Vrkml4YpcKdIaKWUcCl)\n",
        "\n",
        "\n",
        "In this image, you can see the detected hand but stored in the working directory. This is the hand extracted by the algorithm from the whole photo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gh6he6QILKNq",
        "colab_type": "code",
        "outputId": "de9b7500-0e4d-4084-a5ce-b868cce88b44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "#Method to extend (margin) region of a box (area) inside an image (im)\n",
        "def amplifyArea(im, area, margin):\n",
        "    new_area = [0,0,0,0]\n",
        "    new_area[0] = max(area[0]-margin, 0)\n",
        "    new_area[1] = max(area[1]-2*margin, 0)\n",
        "    new_area[2] = min(area[2] + 2*margin, im.shape[1])\n",
        "    new_area[3] = min(area[3] + 2*margin, im.shape[0])\n",
        "    return new_area\n",
        "\n",
        "#Mehtod to pain the black box\n",
        "def draw_rects_filled(img, rects, color, Label):\n",
        "    cv2.putText(img, Label, (rects[0][0], rects[0][1]-10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,0,255), 2)\n",
        "    for x1, y1, x2, y2 in rects:\n",
        "        cv2.rectangle(img, (x1, y1), (x2, y2), color, cv2.FILLED)\n",
        "\n",
        "#Method to store the image in the current working directory\n",
        "def storeHand(hand):\n",
        "  cv2.imwrite('image.png', hand)\n",
        "\n",
        "\n",
        "def camshift_detection():\n",
        "  selection = []\n",
        "  track_window = []\n",
        "  detected = []\n",
        "  #detectedArea = []\n",
        "  detectedface = []\n",
        "  amplified_margin = 50\n",
        "  hsv_roi_selection = []\n",
        "  vis_roi_selection = []\n",
        "  black_box_coordinates = []\n",
        "  track_window_hand = []\n",
        "  face_size = []\n",
        "  first_time = True\n",
        "\n",
        "  VideoCapture()\n",
        "  eval_js('create()')\n",
        "  while True: \n",
        "    first_time = False\n",
        "    #pdb.set_trace()\n",
        "    byte = eval_js('capture()')\n",
        "    im = byte2image(byte)\n",
        "    vis = im.copy()\n",
        "    gray = cv2.cvtColor(im, cv2.COLOR_RGB2GRAY)\n",
        "    hsv = cv2.cvtColor(im, cv2.COLOR_BGR2HSV)\n",
        "    # Mask is configured to do a more \"aggresive\" filter to the hsv. \n",
        "    # Both for the face and the complete image\n",
        "    mask = cv2.inRange(hsv, np.array((0., 60., 32.)), np.array((180., 255., 255.)))\n",
        "    # Detect a face\n",
        "    if selection == [] :\n",
        "      # Detect face\n",
        "      selection = detect(gray)\n",
        "      if selection == [] : \n",
        "        # Don't do anyhting until face detected\n",
        "        continue\n",
        "      else:\n",
        "        # Face detected\n",
        "        #pdb.set_trace()\n",
        "        first_time = True\n",
        "        selection = selection[0]\n",
        "        # Coordinates of face\n",
        "        x0,y0,x1,y1 = tuple(selection)\n",
        "        face_size = (x0, y0, x1-x0, y1-y0)\n",
        "        # Get face distribution (histogram). Crop image to get just the face\n",
        "        hsv_roi = hsv[y0:y1, x0:x1]\n",
        "        mask_roi = mask[y0:y1, x0:x1]\n",
        "        hist = cv.calcHist( [hsv_roi], [0], mask_roi, [16], [0, 180] )\n",
        "        cv.normalize(hist, hist, 0, 255, cv.NORM_MINMAX)\n",
        "        hist = hist.reshape(-1)\n",
        "        # Set black box area in face\n",
        "        drawing_area = (x0, y0, x1, y1)\n",
        "        #pdb.set_trace()\n",
        "        # Track window face is where we will search for the NEXT face position          \n",
        "        track_window_face = (x0,y0, x1-x0, y1-y0)\n",
        "        prob_face = cv.calcBackProject([hsv], [0], hist, [0, 180], 1)\n",
        "        prob_face &= mask\n",
        "        term_crit = ( cv.TERM_CRITERIA_EPS | cv.TERM_CRITERIA_COUNT, 10, 1 )\n",
        "        track_box_face, track_window_face = cv.CamShift(prob_face, track_window_face, term_crit)\n",
        "        xmin,ymin,dx,dy = track_window_face\n",
        "        black_box_coordinates = (xmin, ymin, xmin+dx, ymin+dy)\n",
        "        rects = amplifyArea(hsv, black_box_coordinates, 30)\n",
        "        draw_rects_filled(hsv, [rects], (0, 0, 0), 'Black box')\n",
        "        #pdb.set_trace()\n",
        "\n",
        "        \n",
        "    \n",
        "\n",
        "    if first_time == False:\n",
        "      #pdb.set_trace()\n",
        "      \n",
        "      # Update black box coordinate for next photo\n",
        "      new_selection = amplifyArea(hsv, black_box_coordinates, 50)\n",
        "      x0,y0,x1,y1 = tuple(new_selection)\n",
        "      track_window_face = (x0,y0, x1-x0, y1-y0)\n",
        "      #pdb.set_trace()\n",
        "      hsv_face = hsv[y0:y1, x0:x1]\n",
        "      mask_face = cv2.inRange(hsv_face, np.array((0., 60., 20.)), np.array((180., 180., 150.)))      \n",
        "      #db.set_trace()\n",
        "      mask_face = mask[y0:y1, x0:x1]\n",
        "      prob_face = cv.calcBackProject([hsv], [0], hist, [0, 180], 1)\n",
        "      prob_face &= mask\n",
        "      #prob_face = cv.calcBackProject([hsv_face], [0], hist, [0, 180], 1)\n",
        "      #prob_face &= mask_face\n",
        "      term_crit = ( cv.TERM_CRITERIA_EPS | cv.TERM_CRITERIA_COUNT, 10, 1 )\n",
        "      track_box_face, track_window_face = cv.CamShift(prob_face, track_window_face, term_crit)\n",
        "      xmin,ymin,dx,dy = track_window_face\n",
        "      #pdb.set_trace()\n",
        "      # Check the box is not too big\n",
        "      if (track_window_face[2]>face_size[2]+20 or track_window_face[3]>face_size[3]+20):\n",
        "        selection = []\n",
        "        #print(\"start over\")\n",
        "        continue\n",
        "        # We need to recalculate the face\n",
        "      black_box_coordinates = (xmin, ymin, xmin+dx, ymin+dy)\n",
        "      \n",
        "      draw_rects_filled(hsv, [black_box_coordinates], (0, 0, 0), 'Black box')\n",
        "      \n",
        "\n",
        "    # Now we analyze the image (with the face covered) and look for the hand\n",
        "    \n",
        "    prob = cv.calcBackProject([hsv], [0], hist, [0, 180], 1)\n",
        "    prob &= mask\n",
        "    #Initially we don't know where will the user place the hand\n",
        "    #Therefore the track _Window_hand will be the whole image (with the face covered)\n",
        "    if track_window_hand == []:\n",
        "      track_window_hand = (0,0,vis.shape[0], vis.shape[1])\n",
        "    else:\n",
        "      track_window_hand = amplifyArea(vis, track_window_hand, 50)\n",
        "    term_crit = ( cv.TERM_CRITERIA_EPS | cv.TERM_CRITERIA_COUNT, 10, 1 )\n",
        "    track_box, track_window_hand = cv.CamShift(prob, track_window_hand, term_crit)\n",
        "    xmin,ymin,dx,dy = track_window_hand\n",
        "    detectedArea = (xmin, ymin, xmin+dx, ymin+dy)\n",
        "    \n",
        "    #Now we move on to painting the hand using the face's histogram\n",
        "    #pdb.set_trace()\n",
        "    draw_rects(prob, [detectedArea], (255,0,0), 'Detected hand')\n",
        "\n",
        "    #Uncomment following line if you want to store hand\n",
        "    #storeHand(im[detectedArea[1]:detectedArea[3], detectedArea[0]:detectedArea[2]])\n",
        "    eval_js('showing(\"{}\")'.format(image2byte(prob)))\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "camshift_detection()\n",
        "\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function create(){\n",
              "      div = document.createElement('div');\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      video = document.createElement('video');\n",
              "      video.setAttribute('playsinline', '');\n",
              "\n",
              "      div.appendChild(video);\n",
              "\n",
              "      stream = await navigator.mediaDevices.getUserMedia({video: {facingMode: \"environment\"}});\n",
              "      video.srcObject = stream;\n",
              "\n",
              "      await video.play();\n",
              "\n",
              "      canvas =  document.createElement('canvas');\n",
              "      canvas.width = video.videoWidth;\n",
              "      canvas.height = video.videoHeight;\n",
              "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
              "\n",
              "      div_out = document.createElement('div');\n",
              "      document.body.appendChild(div_out);\n",
              "      img = document.createElement('img');\n",
              "      div_out.appendChild(img);\n",
              "    }\n",
              "\n",
              "    async function capture(){\n",
              "        return await new Promise(function(resolve, reject){\n",
              "            pendingResolve = resolve;\n",
              "            canvas.getContext('2d').drawImage(video, 0, 0);\n",
              "            result = canvas.toDataURL('image/jpeg', 0.8);\n",
              "            pendingResolve(result);\n",
              "        })\n",
              "    }\n",
              "\n",
              "    function showing(imgb64){\n",
              "        img.src = \"data:image/jpg;base64,\" + imgb64;\n",
              "    }\n",
              "\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:48: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-be6f2aad7674>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m \u001b[0mcamshift_detection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-be6f2aad7674>\u001b[0m in \u001b[0;36mcamshift_detection\u001b[0;34m()\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;31m#Uncomment following line if you want to store hand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;31m#storeHand(im[detectedArea[1]:detectedArea[3], detectedArea[0]:detectedArea[2]])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m     \u001b[0meval_js\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'showing(\"{}\")'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage2byte\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result)\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     if (reply.get('type') == 'colab_reply' and\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3zvCCsHNtdl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}